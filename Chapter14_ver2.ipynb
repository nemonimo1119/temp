{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 14 データサイエンティスト中級者への道"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この章では、データサイエンティスト入門レベルを卒業し、中級者の道に進むためのさまざまな分析の手法やアプローチ、ツールについて紹介します。\n",
    "\n",
    "一部実装もありますが、最低限の基礎的な説明や概念の紹介が中心となります。深層学習を学ぶために必要となる基礎知識、Pythonの処理を高速化するためのツール群、膨大なデータをサーバーに分散処理させるためのSparkの紹介など、今後データ分析をする上で身につけておきたいスキルばかりです。\n",
    "\n",
    "はじめは理解しにくい箇所もあるかもしれませんが、このようなアプローチやツールがあるということを知っておくだけでも、今後の学習にきっと役に立つはずです。\n",
    "\n",
    "なお、本章は基本的に紹介のみですので、用語の解説やサンプルコードは少ししかありません。ここで紹介した手法やツール等については、あくまでリファレンス程度のものです。これらのことを本格的に学びたい場合は、本章だけのコンテンツでは不十分です。参考文献等を読んだり、他の講座で勉強して、どんどんスキルを磨いていってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **[14.1 この章の概要](#14.1-この章の概要)**\n",
    "<br><br>\n",
    "- **[14.2 深層学習を学ぶための準備](#14.2-深層学習を学ぶための準備)**\n",
    "    - [14.2.1 パーセプトロン](#14.2.1-パーセプトロン)\n",
    "    - [14.2.2 ニューラルネットワーク](#14.2.2-ニューラルネットワーク)\n",
    "    - [14.2.3 確率的勾配降下法と誤差逆伝播法](#14.2.3-確率的勾配降下法と誤差逆伝播法)\n",
    "    - [14.2.4 深層学習のパッケージ](#14.2.4-深層学習のパッケージ)\n",
    "<br><br>\n",
    "- **[14.3 Pythonの高速化](#14.3-Pythonの高速化)**\n",
    "    - [14.3.1 高速化するためのツール](#14.3.1-高速化するためのツール)\n",
    "    - [14.3.2 並列処理](#14.3.2-並列処理)\n",
    "    - [14.3.3 Numba入門](#14.3.3-Numba入門)\n",
    "    - [14.3.4 Cython入門](#14.3.4-Cython入門)\n",
    "<br><br>\n",
    "- **[14.4 Spark入門](#14.4-Spark入門)**\n",
    "    - [14.4.1 Sparkとは](#14.4.1-Sparkとは)\n",
    "    - [14.4.2 PySpark入門](#14.4.2-PySpark入門)\n",
    "    - [14.4.3 SparkSQL](#14.4.3-SparkSQL)\n",
    "<br><br>\n",
    "- **[14.5 その他の数学的手法とエンジニアリングツール](#14.5-その他の数学的手法とエンジニアリングツール)**\n",
    "    - [14.5.1 数学的手法](#14.5.1-数学的手法)\n",
    "    - [14.5.2 エンジニアリングツール](#14.5.2-エンジニアリングツール)\n",
    "<br><br>\n",
    "- **[14.6 総合問題](#14.6-総合問題)**\n",
    "    - [14.6.1 総合問題1](#14.6.1-総合問題1)\n",
    "    - [14.6.2 総合問題2](#14.6.2-総合問題2)\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.1 この章の概要\n",
    "ゴール：データサイエンティスト中級者になるための様々なアプローチやツールを知る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この章ではまず、最近注目されている深層学習（ディープラーニング）を学ぶための前準備として、いくつか抑えておきたい基礎概念や実装を紹介します。この講座で学んだ統計知識（最尤法、最小二乗法など）や機械学習の考え方（教師あり学習、教師なし学習、交差検証法、混同行列など）と、ここで学ぶ概念（パーセプトロン、勾配降下法、誤差逆伝播法など）を勉強しておけば、深層学習を学ぶ前の良い準備となるでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、機械学習や深層学習等のモデリングの話から離れて、Pythonの処理を高速化するための方法をいくつか紹介します。\n",
    "\n",
    "モデル作成の前に、データ加工をする必要（いわゆる前処理）が多々ありますが、その前処理の計算時間を改善することも大事です。もちろん、コーディングスキルによって、計算処理時間は異なってくるのですが、そのアルゴリズム作成にも限界があります。ここで紹介するライブラリ等を使うことで、計算コストを下げることができます。\n",
    "\n",
    "一昔前、Pythonはスクリプト言語だから遅いという指摘もありました。ただ、昨今、Cythonや並列処理などの色々なライブラリが出てきており、下手なCプログラムを書くよりも、Pythonで実装したほうがいいこともあります。すべての高速化処理について紹介することはできず、ここで紹介する実装が必ず最適というわけではありませんが、計算時間に課題があったときには、ぜひここで使うライブラリ等を使って計算速度をあげることも検討してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに、分散処理をするためのSparkについて説明します。\n",
    "\n",
    "ご承知の通り、現代の世の中には大量のデータがあり、そのデータは日々蓄積されています。この講座でもある程度、こうしたビッグデータに対応できるようなスキルを身に付けるため、いろいろなツールやアプローチを紹介してきました。ここではさらに、そうした膨大なデータに対して、データの加工から機械学習まで一貫して処理し、複数のサーバーを使った分散処理で計算スピードを上げ、さらにリアルタイムに分析ができるSparkを紹介します。今回は入門ということで、その機能と使い方をみていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "そして最後に、今後データ分析業務をやっていく上で、必要となってくるであろう数学的な手法や、エンジニアリングツール等を紹介します。\n",
    "\n",
    "数学的な手法では、実験計画法やMCMC、エンジニアリング面では、Linuxやクラウドサービスなど簡単に紹介します。もちろん、ここですべてが網羅されているわけではありませんが、ビジネスの現場で使われている手法やツールですので、ぜひ参考にしてください（ただし、これらの手法やツールにとらわれることなく、課題に対する理解や最適なアプローチは何であるか、常に考えていくことが大事だということは忘れないでください）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.2 深層学習を学ぶための準備\n",
    "ゴール：深層学習を学ぶための基礎知識をおさえる\n",
    "\n",
    "深層学習は、ニューラルネットワークを用いた学習の応用です。真相学習を学ぶにあたっては、その基本となる論理回路やパーセプトロンなどの基礎を理解しておきましょう。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2.1 パーセプトロン\n",
    "キーワード：パーセプトロン、論理回路"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは、ニューラルネットワークや深層学習の基礎となるパーセプトロンについて学びましょう。パーセプトロンは、複数の値を受け取って処理をし、1つの結果を返す部品です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AND論理回路の例\n",
    "例として、ANDの論理回路（論理ゲート）の実装を考えます。AND関数は、2つの数値0か1がインプットとなって、どちらの入力も1の場合に1を返す関数です。以下の参照URLが参考になります。\n",
    "\n",
    "AND関数の回路記号や真理表（入力0と1の演算と出力を対応させた表）は一番上の行にあります。真理表の具体的な見方は、たとえば、AとBが両方とも1の場合に、アウトプットとなるYが1になるというのが、表からわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![comment](https://image.jimcdn.com/app/cms/image/transf/dimension=661x10000:format=png/path/s9a246d2d2c830e8d/image/i77bdab51b44d4889/version/1424716737/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参照URL:https://image.jimcdn.com/app/cms/image/transf/dimension=661x10000:format=png/path/s9a246d2d2c830e8d/image/i77bdab51b44d4889/version/1424716737/image.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### パーセプトロンでAND関数を作る\n",
    "\n",
    "以下に示すのはAND関数をパーセプトロンとして構成した例です。ある閾値（theta）とインプット値に対する重み（w1,w2）が設定されており、それらのインプット値と重み付けの演算結果(tmp)がその閾値を超えるかどうかで結果を判定をしています。演算は内積の形になっています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def and_func(x1,x2):\n",
    "    w1,w2,theta = 0.5,0.5,0.7\n",
    "    tmp = x1 * w1 + x2 * w2\n",
    "    if tmp <= theta:\n",
    "        return 0\n",
    "    elif tmp > theta:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の関数の実行結果は以下となり、たとえば、インプットを0と1にしたときのアウトプットは0、インプットを1と1にしたときのアウトプットは1となり、結果はAND演算のようになっていることがわかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 かつ 0: 0\n",
      "0 かつ 1: 0\n",
      "1 かつ 0: 0\n",
      "1 かつ 1: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"0 かつ 0:\",and_func(0,0))\n",
    "print(\"0 かつ 1:\",and_func(0,1))\n",
    "print(\"1 かつ 0:\",and_func(1,0))\n",
    "print(\"1 かつ 1:\",and_func(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記はとてもシンプルな実装で、特定の重みで入力値を0と1に限定すると先ほどのような論理ゲートを模倣できますが、複数のパーセプトロンをつなぎ合わせることで、ニューラルネットワークを構築できます。\n",
    "\n",
    "また、上の参照URLをみていただくとわかる通り、他には、OR関数やNOT関数などもありますので、以下の参考文献などを見て実装してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**[やってみよう]**\n",
    "\n",
    ">OR関数やNAND(NOT AND)関数はどのように実装しますか。考えて、実装してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装』(斎藤 康毅  (著),オライリージャパン)\n",
    "\n",
    ">『機械学習スタートアップシリーズ これならわかる深層学習入門 (KS情報科学専門書)』(瀧 雅人 (著),講談社)\n",
    "\n",
    ">『深層学習 (機械学習プロフェッショナルシリーズ) 』(岡谷 貴之  (著),講談社)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <練習問題 1>\n",
    "\n",
    "NOT AND 関数を作成して、それがあっているかどうか確かめてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2.2 ニューラルネットワーク\n",
    "キーワード：ニューラルネットワーク、活性化関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "パーセプトロンを学んだら、ニューラルネットワークを学びましょう。\n",
    "\n",
    "ニューラルネットワークは、順伝播型ネットワークやフォワードネットワークともいわれ、層状に並べたユニットが、隣り合った層と結合しており、入力情報から出力情報に一方的に伝播していくモデルです。先ほどのパーセプトロンでは2つの入力値を使ってシンプルな条件式で判定して出力を決定していましたが、ニューラルネットワークでは活性化関数と言われる関数を使って計算して、出力を決定します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の参照URLを参考にしてください。上側の図では、入力値X1からXnが、W1からWnによって重みづけされ（掛け算され）、fを関数として計算をして、その結果を出力します。下側の図は、層が複数になっているイメージです。\n",
    "\n",
    "/* 活性化関数と出力関数は同じものか違うものか。同じであれば、下の図中の「出力関数」は「活性化関数」と記述すべきです */"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![comment](https://thinkit.co.jp/images/article/30/2/3021.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参照URL:https://thinkit.co.jp/images/article/30/2/3021.gif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "活性化関数には、ロジスティックシグモイド関数、正規化線形関数等が使われます。詳細は、上で紹介した参考文献『深層学習 (機械学習プロフェッショナルシリーズ) 』(岡谷 貴之  (著),講談社)や、以下の参照URLなどを見てください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![comment](https://image.slidesharecdn.com/deep-learning-20140130-140130205750-phpapp01/95/deep-learning-12-638.jpg?cb=1391115802)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参照URL:https://image.slidesharecdn.com/deep-learning-20140130-140130205750-phpapp01/95/deep-learning-12-638.jpg?cb=1391115802"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2.3 確率的勾配降下法と誤差逆伝播法\n",
    "キーワード：勾配（降下）法、バッチ学習、ミニバッチ学習、確率的勾配降下法、誤差逆伝播法、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本講座の確率統計の章や機械学習の章では、予測した目的関数の値と実際の値のずれ、すなわち誤差をできる限り小さくするために、誤差関数に最小二乗法を適応するアプローチ等を学んできました。私たちがやりたいのは、この誤差関数を最小化する値（パラメータ）を求めることです。そのアプローチ方法をさらに詳しく紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 勾配法\n",
    "\n",
    "誤差関数を最小化する値を探すとき、簡単な関数（二次関数の放物線など）だと求めやすいのですが、複雑な関数になってくると、どこで最小値を取るかを解析的に求めることが困難なケースがほとんどです。\n",
    "\n",
    "そのようなときに、まずはどこかに値をとって、値が小さくなる方向の勾配を求めて最小値を探していくのが**勾配法**です。これを繰り返し計算し、最小値がどこにあるのか探していきます。傾きがその方向を決めるため、数学的には関数の偏微分を求めます。なお、最小値を求めるために下に進めていくことが多いので、勾配降下法といったりもします。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 確率的勾配降下法\n",
    "\n",
    "さてデータを使って学習させるのに、訓練データをすべて使って一気に学習をさせる方法を**バッチ学習**といいます。従来はこのアルゴリズムが採用されていました。しかし、今日のデータ状況からみて、膨大なデータをすべて使って学習させるのには、かなりのコスト（時間やお金）がかかります。\n",
    "\n",
    "そこで、訓練データの中から無作為にデータを選んで、そのデータを使って学習をしていく（重み付けしていく）方法である**ミニバッチ学習**が使われています。このアプローチを取れば、大量なデータをすべて処理する必要がなくなり、計算コストを下げることができます。（なお、データ1つ1つを取り出して学習させる**オンライン学習**もありますが、ここでは省略します。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**確率的勾配降下法**は、このミニバッチ学習（訓練データのサンプルを一部（1つだけでも可））を使って、勾配降下法でパラメータの更新をしていく法です。すべてのデータを使っていないため、計算効率が向上し、さらに局所的な解になってしまうリスクを抑えることができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 誤差逆伝播法\n",
    "\n",
    "ただし、確率的勾配降下法であっても改善できるとはいえ、計算時間がかかることもあります。それを解決する方法として、**誤差逆伝播法**があります。これは、重み付けのパラメータの勾配計算を効率良く計算する手法です。逆伝播とは、出力層側から入力層に誤差情報を伝えていくことをいい、誤差逆伝播法はこのアプローチをとります。上記の参照URLの図にて、下の後ろ向き演算がそのイメージです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で、概念的な紹介は終わりです。同じく、詳細を知りたい方は、上で紹介した参考文献『ゼロから作るDeep Learning ―Pythonで学ぶディープラーニングの理論と実装』(斎藤 康毅  (著),オライリージャパン)等を見てください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.2.4 深層学習のパッケージ\n",
    "キーワード：chainer, theano,pytorch,tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、これまでは深層学習の前学習ということで、パーセプトロンや確率的勾配降下法などを学びました。この講座の後に深層学習を学ぶ方は、今後\n",
    "chainerやtheano、tensorflowという深層学習の計算を実行するためのライブラリを使っていくことになりますので、ここで少し紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chainer\n",
    "\n",
    "株式会社Preferred Infrastructure/Networksが開発するディープラーニングのフレームワークです。いろいろなニューラルネットワークの構造に対応しており、GPUもサポートしています。詳細は、以下のサイトをご覧ください。\n",
    "\n",
    ">[参考URL]\n",
    "\n",
    ">http://chainer.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- theano\n",
    "\n",
    "モントリオール大学のBengio教授によって開発されている数値計算をするためのライブラリです。ディープラーニング自体の実装ではなく、それを計算するためのいろいろなサポート（微分計算、コンパイラ）などをしています。こちらもGPUで使うことができます。(2017年11月の1.0リリースを以て開発が終了しています。)\n",
    "\n",
    ">[参考URL]\n",
    "\n",
    ">http://deeplearning.net/software/theano/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pytorch\n",
    "\n",
    "Pytorchは2つの特徴をもったPythonのパッケージで、1つ目がGPUを使ったテンソル計算ができることと、2つ目が深層学習の計算ができるということです。深層学習のパッケージとしては後発ですが、人気がでているようです。\n",
    "\n",
    ">[参考URL]\n",
    "\n",
    ">https://pytorch.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tensorflow\n",
    "\n",
    "Googleがリリースした機械学習やディープラーニングのライブラリです。様々なOSに対応しています。詳細は以下のURLにありますので、参考にしてください。\n",
    "\n",
    ">[参考URL]\n",
    "\n",
    ">https://www.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で、深層学習を学ぶための準備を終わります。近年は深層学習に関する理論的な本や実装に関する良い本も出ています。以下をお勧めします。この講座を終えられた方ならスムーズに入れるのではないかと思います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『詳解 ディープラーニング ~TensorFlow・Kerasによる時系列データ処理~』(巣籠 悠輔 (著),マイナビ出版)\n",
    "\n",
    ">『PythonとKerasによるディープラーニング』(Francois Chollet (著), 巣籠 悠輔  (その他), 株式会社クイープ (翻訳),マイナビ出版)\n",
    "\n",
    ">『scikit-learnとTensorFlowによる実践機械学習』（Aurelien Geron (著),Oreilly & Associates Inc）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、深層学習については以下の本がとても有名ですので、紹介します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『深層学習』(Ian Goodfellow (著), Yoshua Bengio (著), Aaron Courville (著), 岩澤 有祐 (監修), 鈴木 雅大 (監修), & 9 その他、KADOKAWA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上、翻訳本がほとんどですが、英語版だとネット上に無料で読めるサイトもありますので、参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.3 Pythonの高速化\n",
    "ゴール：Pythonを高速化するための方法を知る"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここからは、Pythonの処理を高速化するためのツールやアプローチについて学びます。Pythonを高速化するにはさまざまな方法がありますが、ここでは、並列処理をするmultiprocessing、コンパイラによるNumba、Cython等について説明します。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythonの高速化については、以下の参考文献があります。1つ目の『ハイパフォーマンスPython』は、全体的なコンピューターシステム視点でPython処理のボトルネックとなる箇所を探すために、プロファイリング（システムをテストして遅い箇所を特定する）を実施したり、テクニカルな視点（リストとタプルの違いと扱い方など）でも説明がされています。2つ目の『科学技術計算のためのPython入門』も高速化について、いくつかの章で解説がされていますので、参考にしてください。3つ目の『エキスパートPythonプログラミング改訂2版』にも、最適化や並行処理について述べられているので、こちらも参考にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『ハイパフォーマンスPython』(Micha Gorelick (著), Ian Ozsvald (著), 相川 愛三 (翻訳),オライリージャパン)\n",
    "\n",
    ">『科学技術計算のためのPython入門』(中久喜健司,技術評論社)\n",
    "\n",
    ">『エキスパートPythonプログラミング改訂2版』(Michal Jaworski (著), Tarek Ziade (著), 稲田 直哉 (翻訳), 芝田 将 (翻訳), 渋川 よしき (翻訳), 清水川 貴之  (翻訳), 森本 哲也 (翻)、KADOKAWA）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ただし、Pythonによる高速化や処理の最適化は、まず動くコードを作成したあとに考えるのが原則です。システム全体として最適化をするためには、まずその全体的な動きをつかむことが重要で、局所的に最適化しても全体として最適化できるとは限りません。目的となるシステム、処理を作成した後、計算スピードに問題があるときに、検討するようにしましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**なお、環境等によって、計算時間が変わってきますので、記載通りの結果にならないこともありますが、ご了承ください。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3.1 並列処理\n",
    "キーワード：並行処理、並列処理、プロセス、スレッド、GIL、multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python高速化のために、まずmultiprocessingのモジュールを使った処理をみてきます。\n",
    "\n",
    "その前に、周辺で必要となる概念（並列処理と並行処理、スレッドとプロセス、GILなど）について紹介します。ここでは、簡単な用語の説明に留めるので、詳細は先ほど紹介した参考文献等をみてください。\n",
    "\n",
    "#### 並列処理と並行処理\n",
    "\n",
    "まずは並列処理と並行処理の違いからです。並列処理と並行処理は同じような用語ですが、別の概念です。並行処理は複数のタスクを非同期で実行していきます。一方、並列処理は、並行処理の1つであり、複数のCPUを使って複数のタスクを同時に処理をしてきます。\n",
    "\n",
    "言葉の説明だけでは若干イメージがわきにくいので、以下の参照図をみてみましょう。左の図が並列処理（Parallel）で、右の図が並行処理（Concurrent）です。並列処理では、CPU1とCPU2の2つのCPUがそれぞれ同時に異なるタスクを処理していますが、並行処理では1つのCPUが2つのタスクを非同期で実行しているのがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![comment](http://www.dotnetcurry.com/images/dotnetcore/concurrent/parallel-vs-concurrent-dotnet-core.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参照URL:http://www.dotnetcurry.com/dotnet/1360/concurrent-programming-dotnet-core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "複数のCPUコアを使うときに意識しなければいけないのが、**グローバル・インタプリタ・ロック（GIL）**です。これは、Pythonのプロセスはコア数に関わらず、一度に１つの命令しか実行しないことを言います。\n",
    "\n",
    "しかし以下で紹介するmultiprocessingの並列処理を使えば、プロセスとスレッドを使った並列処理を実現することができ、1つのマシンで複数のコアを使うことができます。(プロセスとは、プログラムの実行単位のことで、スレッドとは、プロセスで作成される処理の単位のことをいいます。)\n",
    "\n",
    "#### multiprocessingを使った並列処理の例\n",
    "\n",
    "用語の説明はここまでにして、multiprocessingのサンプルコードを実際にみていきましょう。\n",
    "\n",
    "以下のコードは、インプットの値を2乗してそれを表示し、0.5秒待つ処理をするものです。このプログラムは何も特別な処理をせず、普通に実装しているだけのコードです(ただし、あくまで並列処理のイメージをもってもらうための実装で、これがシステム的に最適であるとは限りません。)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "4\n",
      "9\n",
      "16\n",
      "25\n",
      "36\n",
      "49\n",
      "64\n",
      "81\n",
      "CalcTime: 5.036837100982666\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# インプットを二乗して結果を表示し、0.5秒待つ\n",
    "def calc(x):\n",
    "    a = x**2\n",
    "    print(a)\n",
    "    # 0.5秒待つ\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# ここから処理を始める\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # 計算開始時間\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # リストデータの作成\n",
    "    data = [t for t in range(0,10)]\n",
    "\n",
    "    # 関数の呼び出しとリスト化\n",
    "    [calc(x) for x in data]\n",
    "    \n",
    "    # 計算時間の測定\n",
    "    print(\"CalcTime:\",time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の実装では、計算時間は約5秒です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、multiprocessingを使って並列処理してみましょう。multiprocessing.Process等を使って、次のように実装します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "9\n",
      "49\n",
      "25\n",
      "1\n",
      "16\n",
      "36\n",
      "64\n",
      "4\n",
      "81\n",
      "CalcTime: 1.5637259483337402\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import multiprocessing\n",
    "\n",
    "# インプットデータから以下のcalcを使ってリスト化する関数\n",
    "def worker(data):\n",
    "    [calc(x) for x in data]\n",
    "\n",
    "# インプットを二乗して結果を表示し、0.5秒待つ\n",
    "def calc(x):\n",
    "    a = x ** 2\n",
    "    print(a)\n",
    "    time.sleep(0.5)\n",
    "\n",
    "# ここから処理開始\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # プロセスを分けるための設定\n",
    "    split_data = [[0, 1, 2], [3, 4],[5, 6],[7, 8, 9]]\n",
    "\n",
    "    jobs = []\n",
    "    for data in split_data:\n",
    "        job = multiprocessing.Process(target=worker, args=(data, ))\n",
    "        jobs.append(job)\n",
    "        job.start()\n",
    "\n",
    "    [job.join() for job in jobs]\n",
    "    \n",
    "    print(\"CalcTime:\",time.time() - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算時間は約1.5秒に改善しました。\n",
    "\n",
    "#### マルチスレッドを使った例\n",
    "\n",
    "次に、マルチスレッドを使った処理を見ていきましょう。以下の処理では3つのURLにリクエストを送って、待ち時間の合計を計算しています。こちらは普通の実装です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.252300977706909\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import time\n",
    "\n",
    "current_time = time.time()\n",
    "urls = ['http://www.google.com', 'http://www.yahoo.co.jp/', 'https://www.bing.com/']\n",
    "for url in urls:\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "print(time.time() - current_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上の結果から、約2.6秒かかったことがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次にこれをマルチスレッドを使って処理してみましょう。threadingを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.google.com: 0.2912478446960449\n",
      "https://www.bing.com/: 0.4851679801940918\n",
      "http://www.yahoo.co.jp/: 0.927203893661499\n",
      "Time: 0.9304249286651611\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import threading, time\n",
    "\n",
    "def get_html(url):\n",
    "    current_time = time.time()\n",
    "    response = urlopen(url)\n",
    "    html = response.read()\n",
    "    print(url + ': ' + str(time.time() - current_time))\n",
    "\n",
    "urls = ['http://www.google.com', 'http://www.yahoo.co.jp/', 'https://www.bing.com/']\n",
    "threads = []\n",
    "\n",
    "# Start Threads\n",
    "current_time = time.time()\n",
    "for url in urls:\n",
    "    thread = threading.Thread(target=get_html, args=(url,))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "# Wait Threads\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "print('Time: ' + str(time.time() - current_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "処理時間は約1秒になり、先ほどより改善されているのがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参照URL]\n",
    "\n",
    ">http://news.mynavi.jp/series/python/032/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3.2 Numba入門\n",
    "キーワード：JITコンパイラ,numba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、JIT(Just in Time)コンパイラを使って、高速化しましょう。ここでは、LLVMベースのコンパイラ**numba**を使います。\n",
    "高速化するには、高速化したい関数の前にデコレータ（入力として関数を受け取り別の関数を返す）の@jitをつけます。そうすることでJITコンパイラが機械語にコンパイルしてくれて、Cにも匹敵する計算性能が出せるようになります。\n",
    "\n",
    "以下に示すのは、複素数の計算をする例です。ふつうにPythonの機能として実装したmulti_abs_basicとNumpyで実装したmulti_abs_numpy、そして、先頭に「@jit」を付けて、multi_abs_basicと同じ処理をJIT化したmulti_abs_numbaの3つの関数を用意しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "#それぞれの要素の掛け算を実行して、リストにする\n",
    "#普通の実装\n",
    "def mult_abs_basic(N,x,y):\n",
    "    r = []\n",
    "    for i in range(N):\n",
    "        r.append(abs(x[i] * y[i]))\n",
    "    return r\n",
    "\n",
    "#numpyの実装\n",
    "def mult_abs_numpy(x,y):\n",
    "    return np.abs(x*y)\n",
    "\n",
    "#JITの実装\n",
    "@jit('f8[:](i8, c16[:], c16[:])',nopython=True)\n",
    "def mult_abs_numba(N,x,y):\n",
    "    r = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        r[i] = abs(x[i] * y[i])\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは、計算をするための変数の準備をします。以下のJは複素数を扱うために使っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N = 1000000\n",
    "\n",
    "x_np = (np.random.rand(N)-0.5) + 1J*(np.random.rand(N)-0.5)\n",
    "y_np = (np.random.rand(N)-0.5) + 1J*(np.random.rand(N)-0.5)\n",
    "\n",
    "x = list(x_np)\n",
    "y = list(y_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それではまず、普通に計算したときの処理時間を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calc Time:0.318[s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "b1 = mult_abs_basic(N,x,y)\n",
    "print('Calc Time:{0:0.3f}[s]'.format(time.clock()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "処理時間は約0.3秒でした。次は、numpyを使って計算した場合です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calc Time:0.047[s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "b1 = mult_abs_numpy(x_np,y_np)\n",
    "print('Calc Time:{0:0.3f}[s]'.format(time.clock()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算時間は、約0.044秒でした。以前学んだ通り、numpyを使った方が計算時間が大幅に改善されているのがわかります。\n",
    "\n",
    "最後にJITコンパイラを使って計算した時を見てみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calc Time:0.010[s]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.clock()\n",
    "b1 = mult_abs_numba(N,x_np,y_np)\n",
    "print('Calc Time:{0:0.3f}[s]'.format(time.clock()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算時間は0.01秒になっています。Cで実装されているNumpyよりもかなり計算時間が削減されていることがわかります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『科学技術計算のためのPython入門』(中久喜健司,技術評論社)\n",
    "\n",
    ">http://qiita.com/kenmatsu4/items/7c08a85e41741e95b9ba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.3.3 Cython入門\n",
    "キーワード：Cython、コンパイル言語、インタープリタ言語"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pythonの高速化の最後として、**Cython**を使ったサンプルコードを紹介します。\n",
    "\n",
    "PythonとCとの違いは、Pythonがインタープリタ言語に対して、Cはコンパイラ言語であるという点です。Pythonのようなインタープリタ言語はすぐに実行、その処理結果の確認ができますが、C言語は機械語に直す過程（コンパイル）が必要であり、時間がかかります。一方、インタープリタ言語のデメリットとしては、実行速度が遅くなってしまう点が挙げられます。コンパイラ言語は、機械語に置き換えるため、実行速度が速くなるからです。\n",
    "\n",
    "ここで扱うCythonは、Pythonのように簡単に実装でき、コンパイルは必要になりますが、実行速度を早めることができます。\n",
    "\n",
    "CythonはNumpyやScipyなどの様々なパッケージで使われており、本来は背景やその仕組み（CPythonとの関係性等）についても詳しく記載すべきですが、本書での目的は、まずは実装して動かすということに重きをおいているために、省略します。詳細は、以下で紹介する参考文献をみてください。\n",
    "\n",
    "#### PythonとCythonの速度を比較する\n",
    "\n",
    "ここでは、PythonとCythonの計算スピードの比較をします。まずは、Jupyter Notebookにて、以下のようにマジックコマンドを実行します。この%load_ext Cythonと以下で述べる%%cythonを実行することで、jupyter notebook内でもコンパイルが可能になります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは以前扱った素数とフィボナッチ数を計算するプログラムをPythonとCythonで実装し、それぞれの速度の違いを比較してみましょう。\n",
    "\n",
    "#### Pythonによる実装\n",
    "まずは普通のPythonによる実装です。1つ目が以前にも扱ったフィボナッチ数列の生成関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# フィボナッチ数列の作成\n",
    "def pyfib(n):\n",
    "    a, b = 0.0, 1.0\n",
    "    for i in range(n):\n",
    "        a, b = a+b, a\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2つ目は素数生成の関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 素数の作成\n",
    "def pyprimes(kmax):\n",
    "    p = np.zeros(1000)\n",
    "    result = []\n",
    "\n",
    "    # 最大個数は1000個\n",
    "    if kmax > 1000:\n",
    "        kmax = 1000\n",
    "\n",
    "    k = 0\n",
    "    n = 2\n",
    "    while k < kmax:\n",
    "        i = 0\n",
    "        while i < k and n % p[i] != 0:\n",
    "            i += 1\n",
    "\n",
    "        if i == k:\n",
    "            p[k] = n\n",
    "            k += 1\n",
    "            result.append(n)\n",
    "        n += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cythonによる実装\n",
    "\n",
    "次はCythonによる実装です。普通のPythonとの書き方の違いは、手前に%%cython マジックコマンドを記載し、変数の前に型（整数型、倍精度浮動小数点型など）が必要になるという点です。以下に示すように、「cdef int」や「cdef double」などのように、cdefの後に型を書いて、その後ろに変数名を書きます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython -n test_cython_code\n",
    "# フィボナッチ数列生成（Cythonバージョン）\n",
    "def fib(int n):\n",
    "    cdef int i\n",
    "    cdef double a=0.0, b=1.0\n",
    "\n",
    "    for i in range(n):\n",
    "        a, b = a+b, a\n",
    "    return a\n",
    "\n",
    "# 素数生成（Cythonバージョン）\n",
    "def primes(int kmax):\n",
    "    cdef int n, k, i\n",
    "    cdef int p[1000]\n",
    "    result = []\n",
    "\n",
    "    if kmax > 1000:\n",
    "        kmax = 1000\n",
    "\n",
    "    k = 0\n",
    "    n = 2\n",
    "    while k < kmax:\n",
    "        i = 0\n",
    "        while i < k and n % p[i] != 0:\n",
    "            i += 1\n",
    "\n",
    "        if i == k:\n",
    "            p[k] = n\n",
    "            k += 1\n",
    "            result.append(n)\n",
    "        n += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 速度の違いを確認する\n",
    "\n",
    "それでは、普通のコードとCythonのコードを比較してみましょう。まずは、フィボナッチ数列から計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 loops, best of 3: 1.06 µs per loop\n",
      "10000 loops, best of 3: 54.2 µs per loop\n"
     ]
    }
   ],
   "source": [
    "# フィボナッチ計算比較\n",
    "%timeit fib(1000)\n",
    "%timeit pyfib(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計算時間がなんと約50倍も改善しています。次は素数の計算です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 3: 2.05 ms per loop\n",
      "1 loop, best of 3: 290 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# 素数計算比較\n",
    "%timeit primes(1000)\n",
    "%timeit pyprimes(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "こちらは100倍以上の改善となりました。\n",
    "\n",
    "Cythonについては以下の文献が参考になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『Cython ―Cとの融合によるPythonの高速化』(Kurt W. Smith (著), 中田 秀基 (監修), 長尾 高弘  (翻訳),オライリージャパン )\n",
    "\n",
    ">http://qiita.com/kenmatsu4/items/7c08a85e41741e95b9ba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で、Pythonの高速化は終わりです。他にも、Pythonで高速化するためのツールとして、分散処理をするDaskやBlazeなどがありますので、興味のある方は以下の参考URLや参考文献を調べてみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考URL]\n",
    "\n",
    ">http://blaze.pydata.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『Python言語によるビジネスアナリティクス 実務家のための最適化・統計解析・機械学習』(久保 幹雄  (著), 小林 和博 (著), 斉藤 努 (著), 並木 誠 (著), 橋本 英樹 (著),近代科学社)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "冒頭でも述べましたが、ここで紹介した実装が必ず最適というわけではありません。Pythonでの計算に時間がかかる時は、上記のような手法を用いて、高速化できるかどうか検討して試してみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.4 Spark入門\n",
    "ゴール：Sparkの機能について知る、PySparkの基礎的な機能を使える"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sparkは分散処理をするためのツールでビッグデータを解析するためのソフトウェアの1つです。機械学習のライブラリも扱え、しかもSQLのように簡単に集計ができたり、リアルタイムに分析できます。\n",
    "\n",
    "SparkはScalaベースで作られていますが、Scala以外にもPythonやJavaなどからも利用可能です。本講座では、Pythonとの連携をメインとして、その処理方法の基礎を学びます。特にPysparkについて、その簡単な使い方を紹介します。\n",
    "\n",
    "今までは、NumpyやScipy、Pandas、sklearnなどを使って、データの加工処理から機械学習のモデリングを実施してきました。扱ってきたデータもそれほど大きくなく、せいぜい数ギガ程度の大きさでした。しかし、データ分析の現場では、数百ギガやテラバイト級のデータがあり、それを扱う場面も多々あり、先ほどのライブラリだけでは、データが読み込めない、計算が終わらない、困難というケースもあります。\n",
    "\n",
    "そこで、そのような大量のデータの読み込み、加工、機械学習まで一貫して実行するためのSparkを使うことで、ビッグデータを扱うことが可能となります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4.1 PySpark入門\n",
    "キーワード：RDD, key/value, mapreduce, sparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは、PythonとSparkを連携させたPySparkの使い方について、基礎の基礎を学びましょう。PySparkはPythonからSparkを操作できるAPIです。\n",
    "\n",
    "#### Pysparkを使うための準備\n",
    "\n",
    "まずは、PySparkを使うために、以下のコードを実行しましょう。（PySparkを実行するための環境は準備されているものとします。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "import pyspark\n",
    "sc = pyspark.SparkContext(appName=\"myAppName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のようにscコマンドを入力するとパージョン番号が表示されるので、spark(pyspark)が使えるようになったことを確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://172.18.0.2:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>myAppName</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=myAppName>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### データの読み込み\n",
    "\n",
    "分析対象のデータの読み込みをします。sc.textFileの後にデータのある場所を指定しています。なお、ファイル名にアスタリスク(✴︎)をつけて任意の文字を含むファイル名をまとめて読みこむことが可能です。同じような規則性のあるファイル名を扱う時は便利です。確率と統計の章で扱った「student-mat.csv」と「student-por.csv」を同時に読み込んでみましょう。ただし、ここでは重複等は気にせず、そのまま読み込みます。**なお、該当のデータがある場所は絶対パスで指定してください。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merge_student_data = sc.textFile(\"/root/userspace/chap3/student*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/.notebooktemp'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これはRDD（Resilient Distributed Dataset）としてデータをロードさせています。日本語では、「不変・並列実行可能な(分割された)コレクション」という意味です。RDDをベースにSparkはデータを分散処理させます。Sparkのすべての作業は、このようなRDDの生成、既存のRDDの変換、結果を集計するためにRDDに対する呼び出しをしています。この段階ではまだ集計等はしていません。\n",
    "\n",
    "次は、行数を数えています。ここで集計を開始します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1046"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Line count\n",
    "merge_student_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下でデータのはじめの行を表示します。RDDの後にfirst()をつけて実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'school;sex;age;address;famsize;Pstatus;Medu;Fedu;Mjob;Fjob;reason;guardian;traveltime;studytime;failures;schoolsup;famsup;paid;activities;nursery;higher;internet;romantic;famrel;freetime;goout;Dalc;Walc;health;absences;G1;G2;G3'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_student_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take(5)で5行を抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['school;sex;age;address;famsize;Pstatus;Medu;Fedu;Mjob;Fjob;reason;guardian;traveltime;studytime;failures;schoolsup;famsup;paid;activities;nursery;higher;internet;romantic;famrel;freetime;goout;Dalc;Walc;health;absences;G1;G2;G3',\n",
       " '\"GP\";\"F\";18;\"U\";\"GT3\";\"A\";4;4;\"at_home\";\"teacher\";\"course\";\"mother\";2;2;0;\"yes\";\"no\";\"no\";\"no\";\"yes\";\"yes\";\"no\";\"no\";4;3;4;1;1;3;6;\"5\";\"6\";6',\n",
       " '\"GP\";\"F\";17;\"U\";\"GT3\";\"T\";1;1;\"at_home\";\"other\";\"course\";\"father\";1;2;0;\"no\";\"yes\";\"no\";\"no\";\"no\";\"yes\";\"yes\";\"no\";5;3;3;1;1;3;4;\"5\";\"5\";6',\n",
       " '\"GP\";\"F\";15;\"U\";\"LE3\";\"T\";1;1;\"at_home\";\"other\";\"other\";\"mother\";1;2;3;\"yes\";\"no\";\"yes\";\"no\";\"yes\";\"yes\";\"yes\";\"no\";4;3;2;2;3;3;10;\"7\";\"8\";10',\n",
       " '\"GP\";\"F\";15;\"U\";\"GT3\";\"T\";4;2;\"health\";\"services\";\"home\";\"mother\";1;3;0;\"no\";\"yes\";\"yes\";\"yes\";\"yes\";\"yes\";\"yes\";\"yes\";3;2;2;1;1;5;2;\"15\";\"14\";15']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_student_data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、lambdaも組み合わせて、フィルターをします。具体的には、それぞれのラインについて、GPという文字が含まれているものを抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gp_lines = merge_student_data.filter(lambda line: \"GP\" in line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "補足ですが、まだ上のフィルターの段階では、計算はしておらず、上記のcountやfirstを実行した時に計算がされます。これはSparkの**遅延評価**によるものです。先ほどの、take()も実施した時に、計算がされます。Sparkではこの考え方がとても重要なので、抑えておいてください。\n",
    "\n",
    "以下はレコード数をカウントしています。これを実行したときに、計算が実行されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "772"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gp_lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上記の結果より、GPが含まれるレコード数は772であることがわかりました。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、先頭のカラムがあるレコードをカウントしましょう。先ほど2ファイルをそのまま読み込んでいるので、先頭のカラムは2行あるはずです。以下は、カラム名schoolが含まれているレコード数をカウントしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "head_lines = merge_student_data.filter(lambda line: \"school\" in line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_lines.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これを応用して、MapReduceの処理を見てみましょう。以下では、それぞれのセルに入っているデータ（数字も）を1つの単語と見なし、それぞれいくつあるのかをカウントする処理をしています。\n",
    "\n",
    "まず、splitを使って「;」で文字を分けています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split_words = merge_student_data.flatMap(lambda line:line.split(\";\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、mapで単語1つにカウント1を対応させ、reduceの処理でそれぞれ同じ単語のカウントを足しあげています。これがMapReduceの処理です。キーとなっているのが単語(word)です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_counts = split_words.map(lambda word:(word,1)).reduceByKey(lambda a,b:a+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collect()でRDD全体を取り出しますので、大きなデータを扱う際は注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "result = word_counts.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返ってくる値はリスト型です。数が多いので表示結果は絞っています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('24', 2),\n",
       " ('schoolsup', 2),\n",
       " ('\"health\"', 123),\n",
       " ('30', 2),\n",
       " ('Fjob', 2),\n",
       " ('22', 7),\n",
       " ('\"U\"', 759),\n",
       " ('\"4\"', 4),\n",
       " ('G1', 2),\n",
       " ('16', 350),\n",
       " ('54', 1),\n",
       " ('higher', 2),\n",
       " ('G3', 2),\n",
       " ('56', 1),\n",
       " ('\"M\"', 453),\n",
       " ('40', 1),\n",
       " ('romantic', 2),\n",
       " ('\"GP\"', 772),\n",
       " ('internet', 2),\n",
       " ('13', 117)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また、Statisticsライブラリ等を使うことで、基本統計量の計算も可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.stat import Statistics\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "まずは、統計量を計算するためのデータを準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize([Vectors.dense([2, 0, 0, -2]),Vectors.dense([4, 5, 0,  3]),Vectors.dense([6, 7, 0,  8])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に、 Statistics.colStatsを使って、基本統計量等を計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary = Statistics.colStats(rdd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は、順番にベクトルの各要素の平均、分散、ゼロでない数のカウントを計算しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.  4.  0.  3.]\n",
      "[  4.  13.   0.  25.]\n",
      "[ 3.  2.  0.  3.]\n"
     ]
    }
   ],
   "source": [
    "print (summary.mean())\n",
    "print (summary.variance())\n",
    "print (summary.numNonzeros())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で、データの読み込みと簡単な集計の紹介は終わります。次は、SparkSQLを見ていきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <練習問題 1>\n",
    "\n",
    "上のデータmerge_student_dataに対して、schoolsupを含むレコードがどれだけあるかカウントしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.4.3 SparkSQL\n",
    "キーワード：SparkSQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SparkはSQLも扱うことができます。以下は、そのモジュールの読み込みと準備をしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import StructField,StringType,IntegerType,StructType,FloatType\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "確率統計の章で扱ったデータを対象に、データベースで学ぶSQL（SparkSQL）を使って集計してみましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "データを読み込みます。RDDとしてロードしています。なお、該当のデータがある場所は絶対パスで指定してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "student_mat_data = sc.textFile(\"/root/userspace/chap3/student-mat.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ヘッダーを設定するために、ヘッダー部分だけ読み込みましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'school;sex;age;address;famsize;Pstatus;Medu;Fedu;Mjob;Fjob;reason;guardian;traveltime;studytime;failures;schoolsup;famsup;paid;activities;nursery;higher;internet;romantic;famrel;freetime;goout;Dalc;Walc;health;absences;G1;G2;G3'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = student_mat_data.first()\n",
    "header"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "このデータの区切り文字は特殊でした。以下のように;を置き換えましょう。replaceを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schemaString = header.replace(';',',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'school,sex,age,address,famsize,Pstatus,Medu,Fedu,Mjob,Fjob,reason,guardian,traveltime,studytime,failures,schoolsup,famsup,paid,activities,nursery,higher,internet,romantic,famrel,freetime,goout,Dalc,Walc,health,absences,G1,G2,G3'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schemaString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さて、データベースのテーブルを準備するとき、フィールド名や型を全て指定しなければいけませんが、30以上もあるので、設定するのは大変です。以下のようにプログラムを書いて、作業を効率化しましょう。取り急ぎすべて文字型にしています。個別で後で変更も可能です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split(',')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これをデータ構造としてスキーマを定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "schema = StructType(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先ほどのカラムの行（ヘッダー）を除く、データを準備します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "395"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Header = student_mat_data.filter(lambda l: \"school\" in l)\n",
    "NoHeader = student_mat_data.subtract(Header)\n",
    "NoHeader.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下は、カラム名を除いたデータの1行目です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"GP\";\"F\";15;\"R\";\"GT3\";\"T\";1;1;\"at_home\";\"other\";\"home\";\"mother\";2;4;1;\"yes\";\"yes\";\"yes\";\"yes\";\"yes\";\"yes\";\"yes\";\"no\";3;1;2;1;1;1;2;\"7\";\"10\";10'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoHeader.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "また次に、文字が”で囲まれているため、map関数とlambda関数を使って、その文字を消去しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NoHeader2 = NoHeader.map(lambda l: l.replace(\"\\\"\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GP;F;15;R;GT3;T;1;1;at_home;other;home;mother;2;4;1;yes;yes;yes;yes;yes;yes;yes;no;3;1;2;1;1;1;2;7;10;10'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NoHeader2.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "さらに、;で文字が区切られているため、分割しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NoHeader3 = NoHeader2.map(lambda l: l.split(\";\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次は、上のデータをデータフレームにします。先ほどのスキーマで設定します。sqlContext.createDataFrameを使います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame = sqlContext.createDataFrame(NoHeader3, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後に、SQLを使うために、上のデータフレームをテーブルにして、登録します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_frame.registerTempTable(\"tmp_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下で、スキーマを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- school: string (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- famsize: string (nullable = true)\n",
      " |-- Pstatus: string (nullable = true)\n",
      " |-- Medu: string (nullable = true)\n",
      " |-- Fedu: string (nullable = true)\n",
      " |-- Mjob: string (nullable = true)\n",
      " |-- Fjob: string (nullable = true)\n",
      " |-- reason: string (nullable = true)\n",
      " |-- guardian: string (nullable = true)\n",
      " |-- traveltime: string (nullable = true)\n",
      " |-- studytime: string (nullable = true)\n",
      " |-- failures: string (nullable = true)\n",
      " |-- schoolsup: string (nullable = true)\n",
      " |-- famsup: string (nullable = true)\n",
      " |-- paid: string (nullable = true)\n",
      " |-- activities: string (nullable = true)\n",
      " |-- nursery: string (nullable = true)\n",
      " |-- higher: string (nullable = true)\n",
      " |-- internet: string (nullable = true)\n",
      " |-- romantic: string (nullable = true)\n",
      " |-- famrel: string (nullable = true)\n",
      " |-- freetime: string (nullable = true)\n",
      " |-- goout: string (nullable = true)\n",
      " |-- Dalc: string (nullable = true)\n",
      " |-- Walc: string (nullable = true)\n",
      " |-- health: string (nullable = true)\n",
      " |-- absences: string (nullable = true)\n",
      " |-- G1: string (nullable = true)\n",
      " |-- G2: string (nullable = true)\n",
      " |-- G3: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(data_frame.printSchema())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "それでは、SQLのselect文を使ってみましょう。基本的にsqlContext.sqlの中でSQLを記述すれば、大丈夫です。なお、集計結果としては、前半のChapterで実行した結果と同じになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|sex|            avgAge|\n",
      "+---+------------------+\n",
      "|  F| 16.73076923076923|\n",
      "|  M|16.657754010695186|\n",
      "+---+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "count_result = sqlContext.sql(\"select sex,avg(age) as avgAge from tmp_table group by sex\")\n",
    "print(count_result.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で、Sparkの説明は終わりになります。Sparkではさらに、前に学んだ機械学習の手法もMLibを使ってRDDに適応させることが可能です。\n",
    "\n",
    "今回は簡単な実装の紹介のみで、ここの環境ではSparkの凄さを実感できませんが、将来的にAWS（Amazonが提供するクラウドサービス）などでたくさんのサーバーが使えるときや分散処理を実施する時に、ぜひ使ってみてください。ちなみに、インストール等が少し大変だと思うので、AWSが提供するAmazon Elastic MapReduce(EMR）が便利です。ノード数の指定などが簡単に設定できるので、はじめて使う方にはEMRはオススメです。\n",
    "\n",
    "またSparkはScalaベースで作られており、色々な面でScalaからの方が扱いやすいと思いますので、本格的に使われる場合はScalaも選択肢として考えるのも良いと思います（PySparkは制約があります）。\n",
    "\n",
    "なお、参考文献は以下になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『初めてのSpark』(Holden Karau (著), Andy Konwinski (著), Patrick Wendell (著), Matei Zaharia (著), & 1 その他,オライリージャパン )\n",
    "\n",
    ">『入門 PySpark ―PythonとJupyterで活用するSpark 2エコシステム』(omasz Drabas (著),Denny Lee (著),Sky株式会社 玉川 竜司 (翻訳),オライリージャパン )\n",
    "\n",
    ">『Machine Learning with Spark - Tackle Big Data with Powerful Spark Machine Learning Algorithms』(Nick Pentreath (Author),Packt Publishing )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <練習問題 1>\n",
    "\n",
    "SparkSQLを使って、先ほど作ったテーブルに対して、school × sexを軸にして、それぞれのレコード数と、それぞれの平均年齢を表示するようにしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.5 その他の数学的手法とエンジニアリングツール\n",
    "ゴール：データ分析のアプローチ方法(数学的手法とエンジニアリング)を広げる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.5.1 数学的手法\n",
    "キーワード：MCMC, 階層ベイズ、実験計画法、生存解析、確率過程とランダムウォーク、時系列解析、トピックモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "この講座では、データ分析、特に機械学習の分野について、最低限必要な手法を学んできました。さらに、いろいろな課題に対するアプローチを増やすために、数学的な手法とそれらを実装するライブラリ等について紹介します。本書の冒頭で紹介したデータサイエンティストに必要なうちの1つのデータサイエンス（数学・統計モデリング）をスキルアップさせるためのコンテンツ紹介になります。ここでは、ほんの数行程度の説明ですが、この講座終了後に、業務上必要そうなもの、興味があるものをぜひ学んでいってください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **マルコフ連鎖モンテカルロ法（MCMC）**：この手法は乱数を使ってマルコフ連鎖（ある状態が直前の状態にのみ依存して、その連鎖を確率モデルで表したもの）を発生させる方法で、多重積分を計算するときなどに使われます。PythonのライブラリではPyMC3から使うことができます。なお、以前に紹介したEMアルゴリズムと、このMCMCの一種であるギブス標本抽出は関係しています（EステップとMステップ）ので、後で紹介する参考文献等をみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **階層ベイズ**：統計モデルにはパラメータ（母平均など）がありましたが、それに階層構造をもたせてベイズ推定する手法が階層ベイズです。単純にパラメータを固定するだけではうまく現象を説明することができないこともあるため、その背後にある確率分布をさらに考えます。ライブラリは先ほどのPyMC3などを使えば、階層ベイズを使うことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **実験計画法**：ある現象が生じた場合に、その原因は何なのか、それらには本当に因果関係があるのかどうか、実際の実験を通して観察し、その結果が妥当であるかどうかを分析するのが実験計画法です。実験計画法では少ない実験回数で効果的な情報を得られるように実験計画を工夫します。この講座の前半に、相関があるからといって因果関係があるとはいえないという話をしました。この実験計画法によって、ある現象がある要因によって生じているのかどうかを調べていきます。マーケティング分野ではコンジョイント分析という名で利用されています。具体的な例としては、消費者がパソコンを購入する際、価格や色、デザイン、品質などが購入するかしないかに影響してきますが、それらの影響の度合いを調べるための効率的な実験を、直交表という表を用いて計画します。他には、スーパーなどで発行されるクーポンの効果を見るために、ある店舗群はクーポンを配り（処置群）、別の同じような売り上げの店舗群はクーポンを発行しない（コントロール群）という実験等が行われています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **時系列解析**：時間とともにランダムに変動するデータを分析するのが、時系列解析です。分析対象の主なデータは、気候（気温、雨量など）、株価や地震波、売上推移など様々な時系列データです。それらの移動平均を計算したり、それ自身の過去のデータと相関をとって分析（自己相関）したり、多変量の時系列を解析して、将来の予測計算をします。Pythonのライブラリでは、statsmodelsがあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **確率過程とランダムウォーク、確率解析**：ファイナンス理論（オプション価格のデリバティブ計算等）に応用されることが多く、金融業界でクオンツといわれる職種を目指される方は、これらの領域を学ぶことになります。確率微分方程式の離散化バージョンなどは以下の参考文献などに記載されています。以下の参考URL（github）に載せているコンテンツも参考になりますし、また、参考URL（**Quantopian**）もコンテンツがとても充実していますので、金融で必要な実装を学ぶことができ、オススメです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **生存時間解析**：その名の通り、ある分析の対象がどれだけ生き続けているか（人や製品なども含む）を解析するためのアプローチです。生存曲線という、時間と生存する確率を対応させた関数を使いますが、生存時間の分布があらかじめわかっていることは少ないため、それを推定するための手法、カプランマイヤー推定などが使われます。この手法は、あるイベントが発生するまでの時間を解析するための方法で、医療の分野では生存率を評価するときなどに使われているようです。Pythonではlifelinesがパッケージとしてあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **トピックモデル**：自然言語処理の一分野でもあり、ニュースなどたくさんの記事があった時に、それらが一体何のトピックなのか分析するのがトピックモデルです。応用分野としては、ニュース記事の分類以外にも、購買行動の分析（セグメンテーションなど）もあります。Pythonのパッケージでは、gensimがあります。なお自然言語処理は、NLTKパッケージがよく使われます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上、簡単ではありましたが、その他の数学的な手法の紹介は終わります。他にも、さらなる応用領域（ベイジアンネットワーク、状態空間モデルとカルマンフィルタ、マリアバン解析等）もありますので、興味のある方はいろいろと調べてみてください。\n",
    "\n",
    "特にオススメなのが、以下の参照URLの**Quantopian**のウェブサイトです。先ほども少し紹介しました。ファイナンス系のサイトですが、この講座と同様にJupyter notebookkを使って手を動かして学べる環境があります。しかも無料です。ファイナンス専門家の方はもちろん、専門外の方もいろいろと学ぶことが多いと思いますので、オススメです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参照URL]\n",
    "\n",
    ">https://www.quantopian.com/home\n",
    "\n",
    ">https://github.com/wilsonfreitas/awesome-quant\n",
    "\n",
    ">[参考文献]\n",
    "\n",
    ">『Pythonデータサイエンスハンドブック ―Jupyter、NumPy、pandas、Matplotlib、scikit-learnを使ったデータ分析、機械学習』(Jake VanderPlas  (著), 菊池 彰 (翻訳),オライリージャパン)\n",
    "\n",
    ">『IPythonデータサイエンスクックブック ―対話型コンピューティングと可視化のためのレシピ集』(Cyrille Rossant (著), 菊池 彰 (翻訳),オライリージャパン)\n",
    "\n",
    ">『統計的学習の基礎 ―データマイニング・推論・予測』(Trevor Hastie (著), Robert Tibshirani (著), Jerome Friedman (著), 杉山 将  (翻訳), & 22 その他,共立出版)\n",
    "\n",
    ">『パターン認識と機械学習 上下』(C.M. ビショップ  (著), 元田 浩 (監訳), 栗田 多喜夫  (監訳), 樋口 知之 (監訳), & 2 その他, 丸善出版)\n",
    "\n",
    ">『Pythonで体験するベイズ推論 PyMCによるMCMC入門』(キャメロン デビッドソン=ピロン (著), Cameron Davidson‐Pilon (原著), 玉木 徹  (翻訳),森北出版)\n",
    "\n",
    ">『Pythonによるベイズ統計モデリング: PyMCでのデータ分析実践ガイド』(Osvaldo Martin (原著), オズワルド マーティン (著), 金子 武久 (翻訳), 共立出版)\n",
    "\n",
    "\n",
    ">『機械学習スタートアップシリーズ ベイズ推論による機械学習入門 (KS情報科学専門書)』(須山 敦志 (著),杉山 将 (監修)講談社)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.5.2 エンジニアリングツール\n",
    "キーワード：Linux, AWS, hadoop, FPGA, AWS,Scala,R, Java, OpenCV, Tableau, PowerBI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今までは主にPythonを使ってきましたが、分析に関連するソフトウェアや分析に役立つツールを、まばらではありますが紹介します。ここの講座のみですべてを学ぶことはできませんが、将来的なアンテナを張れるように単語を知っておくだけでもためになると思います。ここは、データサイエンティストになるために必要なスキルの2つ目エンジニアリング力を磨くためのコンテンツ紹介になります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Linux（コマンドライン、シェル）**：OSの1つです。この講座でも少しコマンドライン等を使いました。将来、分析をするだけではなく、分析をするための環境構築等にも必要になってくるスキルです。初学者にはなかなかハードルが高いですが、Linuxを徐々に使い慣れていけば、分析環境を構築したり、実際に分析するのに、色々と選択肢が広がるでしょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Hadoop**：分散処理システムです。よく取り上げられる簡単な例が、ある文章の中に出てくる単語をそれぞれカウントするときにどうやって効率よく数えていくのかという問題です。処理としては、文章の中にある単語に数字を対応させ(Map処理)、後から集めて集計する（Reduce処理）という処理をHadoopがやります（分散処理と監視など）。単語のカウントは、上記のSparkの例でも扱いました。また、Sparkでは、このHadoopエコシステムの中で使われることも多いです。ビックデータ分析といえばHadoopというくらい、大量のデータを処理するのによく紹介されます。ただ、こちらはバッチ処理になりますので、リアルタイムに分析をする場合には、Spark等と組み合わせる必要があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **FPGA**：昨今、深層学習計算などでたくさんのCPUやGPUが使われており(他、TPUも)、ソフトウェアの面からだけではなく、ハード面で実装をすることが求められています。このFPGAはプログラム可能なICで、Field Programmable Gate Arrayの略です。論理仕様をプログラムすることができ、ユーザーの思い通りの論理回路を実現できる論理デバイスです。メリットとしては、CPUが処理する段階でロジックを組むことができるので、処理スピードがかなり改善されます。ハードウェア記述言語のVHDLなどで書かれており、始めるのにかなりハードルが高いツールですが、最近はPynqなどPythonからも実装ができるようになってきているようです。FPGAは、データ分析中級者向けというより、レベル的にはかなりの上級者（トップクラスのデータサイエンティストかエンジニア）向けだと思いますが、今後この講座を受けた人の中から、データ分析の第一線で活躍している人が出てくることを期待して、紹介をしました。インフラよりの話ではありますが、Pythonからも使えるようになり、分析をするのにCPUレベルで考えなければいけない時代が来るのかもしれません。応用分野としては、MicrosoftのBingの検索エンジンやゲノム科学解析、ゲームユーザーの振り分け、金融の高頻度取引などに使われており、いろいろな企業が注目しています。ただこれも用途やデータに応じて使うものですので、FPGA（やGPUなど）を使えば必ず速くなるというものではないので注意しましょう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **AWS**：Amazonのクラウドサービスです。サーバー構築やデータストレージ環境など様々なサービスが提供されています。試験的に何か始める時には、コストを抑えることができるため便利です。また、一時的に大量のサーバーを立ち上げるなど、分散処理システムを構築したい場合に使いやすいです。HadoopやSparkがあらかじめインストールされるAmazon EMRや、データベースを分散処理により高速化させるRedshiftなど、分析で使われることも多いです。また、Machine Learning（機械学習）などもありますが、現状は一部だけで、ここで学んだ人には物足りないかもしれません。なお、先ほどのFPGAは、2016年12月からAWSでも使えるようになっています。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **R**：統計ソフトウェアです。統計的な手法のライブラリが豊富です。Pythonのパッケージにはないものも、いくつかあります（変数選択法など他、多数）。もしこれらのパッケージをPythonで利用したい場合は、PythonからRのスクリプトも呼び出すことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **可視化ツール**：Tableau, PowerBIなどがあります。近年はインフォグラフィックスというデータの可視化が注目されており、これらのツールを使うことで、簡単にデータを可視化することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **OpenCV**：画像を処理するためのライブラリです。人の顔を認識したりすることもできます。Pythonと連携が可能で、このJupyterと組み合わせて実行すると便利です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 他：ここで紹介したツールやプログラミング言語の他にも、Java、Rudy、PHP、Scalaや機械学習や統計分析向けのJulia、Jumpも分析用のツールとしてよく使用されているようです。クラウドサービスとしてはAWSの他に、GoogleのGCPやIBMのBlueMix、マイクロソフトのAzureなどがあります。商用の統計解析ツールとしては、SAS、SPSS、Matlabなどがありますが、高額なので一般で買うのは難しいかもしれません。ただ、このような統計ソフトウェアの機能もPythonのライブラリで提供されているものが多くなってきましたので、PythonやRでもほとんど実現可能です。Pythonにはない統計手法を使いたい場合や、これらのソフトウェアに興味のある方、大学や会社で使う必要があるという方は、無償版がありますので、それらをインストールしたり、本などをみて勉強してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">[参考文献]\n",
    "\n",
    ">『ビッグデータ テクノロジー完全ガイド』(Michael Manoochehri (著), 小林 啓倫  (翻訳),マイナビ)\n",
    "\n",
    ">『FPGAの原理と構成』(天野英晴 (編集),オーム社)\n",
    "\n",
    ">[参考URL]\n",
    "\n",
    ">http://qiita.com/kazunori279/items/a9e97a4463cab7dda8b9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上で、この章は終わります。お疲れ様でした。最後に用語の確認だけしましょう。\n",
    "\n",
    "残りは、総仕上げの演習問題となります。なお、総仕上げの問題は基本的に前までの章の知識をベースにしていますので、余裕がある方は取り組んでみてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14.6 総合問題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.6.1 総合問題1\n",
    "\n",
    "深層学習に関する以下の用語について、それぞれの役割やその意味について述べてください。また、ネットや参考文献等も使って調べてみてください。\n",
    "- パーセプトロン\n",
    "- ニューラルネットワーク\n",
    "- 勾配法\n",
    "- バッチ学習\n",
    "- ミニバッチ学習\n",
    "- 確率的勾配降下法\n",
    "- 誤差逆伝播法\n",
    "- Chainer, Theano, TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.6.2 総合問題2\n",
    "\n",
    "Pythonの高速化とSparkに関する用語について、それぞれの役割や意味について述べてください。また、ネットや参考文献等も使って調べてみてください。\n",
    "- 並列処理\n",
    "- JITコンパイル\n",
    "- Cython\n",
    "- Spark\n",
    "- RDD\n",
    "- PySpark\n",
    "- SparkSQL"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
